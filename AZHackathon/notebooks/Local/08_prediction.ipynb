{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../../src\")\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from utils.utils import get_image_metadata\n",
    "from models.unets import UnetResnet152\n",
    "\n",
    "\n",
    "class PredctionDataset(Dataset):\n",
    "    def __init__(self, dir_path, crop_size=(256,256), transform=None):\n",
    "        \"\"\"Prediction dataset for sample images for the Astra Zeneca competition\n",
    "        \n",
    "        Group by row_col and field of view\n",
    "        # row_col\n",
    "        # field of view\n",
    "         \n",
    "        Input and Target share these common values:\n",
    "        - row_col       = sample id? \n",
    "        - field of view = amount of zoom\n",
    "\n",
    "        For identifying INPUT:\n",
    "        - action_list_number A04\n",
    "        - imaging_channel    C04\n",
    "        - z_number_3d        Z01 - Z07\n",
    "\n",
    "        For identifying TARGET:\n",
    "        - action_list_number A01 A02 and A03\n",
    "        - imaging_channel    C01, C02, C03\n",
    "        - z_number_3d        Z01\n",
    "        \"\"\"\n",
    "        self.dir_path = dir_path\n",
    "        \n",
    "        dataset_samples = glob.glob(os.path.join(self.dir_path, \"Assay*\"))\n",
    "\n",
    "        dataset_dicts = [get_image_metadata(path) for path in dataset_samples]\n",
    "        \n",
    "        # Group all 7 inputs with all 3 respective targets into variable sample\n",
    "        samples = dict()\n",
    "        for sample_dict in dataset_dicts:\n",
    "            sample_key = (sample_dict[\"row_col\"], sample_dict[\"field of view\"])\n",
    "\n",
    "            if samples.get(sample_key) is None:\n",
    "                samples[sample_key] = {\n",
    "                    \"input\": dict(),\n",
    "                    \"target\": dict(),\n",
    "                    \"mask\" : dict()\n",
    "                }\n",
    "\n",
    "            if sample_dict[\"action_list_number\"] == \"A04\":\n",
    "                # Is an input\n",
    "                z_number_3d = sample_dict[\"z_number_3d\"]\n",
    "                samples[sample_key][\"input\"][z_number_3d] = sample_dict[\"path\"]\n",
    "            elif sample_dict[\"is_mask\"]:\n",
    "                # Is a mask\n",
    "                action_list_number = sample_dict[\"action_list_number\"]\n",
    "                samples[sample_key][\"mask\"][action_list_number] = sample_dict[\"path\"]\n",
    "            else:\n",
    "                # Is a target\n",
    "                action_list_number = sample_dict[\"action_list_number\"]\n",
    "                samples[sample_key][\"target\"][action_list_number] = sample_dict[\"path\"]                \n",
    "\n",
    "        self.samples = list(samples.values())\n",
    "        self.crop_size = crop_size\n",
    "        self.transforms = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Modulo\n",
    "        idx = idx % len(self.samples)\n",
    "\n",
    "        sample_dict = self.samples[idx]\n",
    "        w, h = cv2.imread(sample_dict[\"input\"][\"Z01\"], -1).shape\n",
    "        assert self.crop_size[0] <= w\n",
    "        assert self.crop_size[1] <= h\n",
    "\n",
    "        input = np.zeros((7, w, h))\n",
    "        output = np.zeros((3, w, h))\n",
    "        mask = np.zeros((3, w, h), dtype = 'int16') # As masks will be binary\n",
    "        input_filenames = list()\n",
    "        for i, z_number_3d in enumerate([\"Z01\", \"Z02\", \"Z03\", \"Z04\", \"Z05\", \"Z06\", \"Z07\"]):\n",
    "            img_path = sample_dict[\"input\"][z_number_3d]\n",
    "            img = cv2.imread(img_path, -1)\n",
    "            input[i] = img\n",
    "            input_filenames.append(os.path.basename(img_path))\n",
    "        \n",
    "        if self.transforms:\n",
    "            for transform in self.transforms: \n",
    "                input = transform(input)\n",
    "                \n",
    "        output_filenames = list()\n",
    "        for target in [\"01\", \"02\", \"03\"]:\n",
    "            tf = input_filenames[0]\n",
    "            target_filename = tf[:43] + 'A' + target + tf[46:49] + 'C' + target + tf[52:]\n",
    "            #target_filename[43:46] = 'A' + target\n",
    "            #target_filename[49:52] = 'C' + target\n",
    "            \n",
    "            output_filenames.append(target_filename)\n",
    "        return input, input_filenames, output_filenames\n",
    "    \n",
    "dataset = PredctionDataset(\"../../data/03_training_data/normalized_bias/train/input/20x_images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 2154, 2554) ['AssayPlate_Greiner_#655090_C04_T0001F003L01A04Z01C04.tif', 'AssayPlate_Greiner_#655090_C04_T0001F003L01A04Z02C04.tif', 'AssayPlate_Greiner_#655090_C04_T0001F003L01A04Z03C04.tif', 'AssayPlate_Greiner_#655090_C04_T0001F003L01A04Z04C04.tif', 'AssayPlate_Greiner_#655090_C04_T0001F003L01A04Z05C04.tif', 'AssayPlate_Greiner_#655090_C04_T0001F003L01A04Z06C04.tif', 'AssayPlate_Greiner_#655090_C04_T0001F003L01A04Z07C04.tif'] ['AssayPlate_Greiner_#655090_C04_T0001F003L01A01Z01C01.tif', 'AssayPlate_Greiner_#655090_C04_T0001F003L01A02Z01C02.tif', 'AssayPlate_Greiner_#655090_C04_T0001F003L01A03Z01C03.tif']\n"
     ]
    }
   ],
   "source": [
    "inputs, filenames, target_filenames = dataset[0]\n",
    "print(inputs.shape, filenames, target_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnetResnet152(input_channels=7, output_channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 2154, 2554])\n",
      "torch.Size([1, 1, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(inputs).unsqueeze(0)\n",
    "print(x.shape)\n",
    "y=model(torch.zeros(1,7,256,64))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_inputs = torch.Tensor(inputs).unsqueeze(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2_weights = \"../../data/05_saved_models/A2_g_best.pth\"\n",
    "a3_weights = \"../../data/05_saved_models/A3_g_best.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_file = a2_weights\n",
    "checkpoint = torch.load(weight_file, map_location=device)\n",
    "epoch = checkpoint['epoch']\n",
    "best_valid_loss = checkpoint['best_valid_loss']\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F2\n",
    "from functools import wraps\n",
    "import time\n",
    "\n",
    "def timeit(my_func):\n",
    "    @wraps(my_func)\n",
    "    def timed(*args, **kw):\n",
    "    \n",
    "        tstart = time.time()\n",
    "        output = my_func(*args, **kw)\n",
    "        tend = time.time()\n",
    "        \n",
    "        print('\"{}\" took {:.3f} ms to execute\\n'.format(my_func.__name__, (tend - tstart) * 1000))\n",
    "        return output\n",
    "    return timed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 608.00 MiB (GPU 0; 39.59 GiB total capacity; 29.05 GiB already allocated; 342.19 MiB free; 29.39 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-75370996100d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0moutput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrided_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m#plt.imshow(output_counts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-272abfa379e4>\u001b[0m in \u001b[0;36mtimed\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-75370996100d>\u001b[0m in \u001b[0;36mstrided_predict\u001b[0;34m(original_inputs, model, device, crop_size, stride, output_channels, batch_size)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mbatch_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Code/AZHackathon/AZHackathon/src/models/unets.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mUnetDpn92\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/segmentation_models_pytorch/encoders/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 608.00 MiB (GPU 0; 39.59 GiB total capacity; 29.05 GiB already allocated; 342.19 MiB free; 29.39 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "@timeit\n",
    "def strided_predict(original_inputs, model, device, \n",
    "                    crop_size:int=256, \n",
    "                    stride:int=256, \n",
    "                    output_channels:int=1, \n",
    "                    batch_size:int=1\n",
    "                   ):\n",
    "    assert crop_size >= stride # Crop size must be larger than stride\n",
    "    \n",
    "    sizes = np.array([2**i for i in range(10)])\n",
    "    b, c, w, h = original_inputs.shape\n",
    "    output_image = torch.zeros(b,output_channels,w,h)\n",
    "    output_counts = torch.zeros(w,h)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        batch_tensors = list()\n",
    "        batch_metadata = list()\n",
    "        \n",
    "        for i in range(0, w-crop_size+stride, stride):\n",
    "            \n",
    "            for j in range(0, h-crop_size+stride, stride):\n",
    "\n",
    "                x = original_inputs[:,:,i:i+crop_size,j:j+crop_size]\n",
    "\n",
    "                # Resize rectangular tensors into allowed rectangular tensors\n",
    "                crop_shape = x.shape\n",
    "\n",
    "                if x.shape[2] != crop_size:\n",
    "                    idx = np.argmin(np.abs(sizes - x.shape[2]))\n",
    "                    x = F.interpolate(x, size=(sizes[idx], x.shape[3]))\n",
    "                if x.shape[3] != crop_size:\n",
    "                    idx = np.argmin(np.abs(sizes - x.shape[3]))\n",
    "                    x = F.interpolate(x, size=(x.shape[2], sizes[idx]))\n",
    "                \n",
    "                batch_metadata.append({\"crop_size\": crop_size, \"crop_shape\": crop_shape, \"i\": i, \"j\": j})\n",
    "                batch_tensors.append(x)\n",
    "                \n",
    "                if len(batch_tensors) == batch_size:\n",
    "                    x_batch = torch.cat(batch_tensors, 0)\n",
    "                    batch_out = model(x_batch.to(device)).detach().cpu()\n",
    "                    \n",
    "                    for out, metadata in zip(batch_out, batch_metadata):\n",
    "                        out = out.unsqueeze(0)\n",
    "                        crop_size = metadata[\"crop_size\"]\n",
    "                        crop_shape = metadata[\"crop_shape\"]\n",
    "                        i = metadata[\"i\"]\n",
    "                        j = metadata[\"j\"]\n",
    " \n",
    "                        out = F.interpolate(out, size=(crop_shape[2], crop_shape[3]))\n",
    "\n",
    "                        output_counts[i:i+crop_size,j:j+crop_size] += torch.ones(crop_shape[2],crop_shape[3])\n",
    "                        output_image[:,:,i:i+crop_size,j:j+crop_size] += out\n",
    "                    batch_out = list()\n",
    "                    batch_metadata = list()\n",
    "                    \n",
    "    if len(batch_tensors) != 0:\n",
    "        x_batch = torch.cat(batch_tensors, 0)\n",
    "        batch_out = model(x_batch.to(device)).detach().cpu()\n",
    "\n",
    "        for out, metadata in zip(batch_out, batch_metadata):\n",
    "            out = out.unsqueeze(0)\n",
    "            crop_size = metadata[\"crop_size\"]\n",
    "            crop_shape = metadata[\"crop_shape\"]\n",
    "            i = metadata[\"i\"]\n",
    "            j = metadata[\"j\"]\n",
    "\n",
    "            out = F.interpolate(out, size=(crop_shape[2], crop_shape[3]))\n",
    "\n",
    "            output_counts[i:i+crop_size,j:j+crop_size] += torch.ones(crop_shape[2],crop_shape[3])\n",
    "            output_image[:,:,i:i+crop_size,j:j+crop_size] += out\n",
    "        batch_out = list()\n",
    "        batch_metadata = list()\n",
    "\n",
    "    output_image = output_image * (1 / output_counts)\n",
    "    return output_image\n",
    "\n",
    "output_image = strided_predict(original_inputs, model, device, crop_size=256, stride=128)\n",
    "\n",
    "#plt.imshow(output_counts)\n",
    "#plt.imshow(output_image[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@timeit\n",
    "def test_time_augmentation_predict(inputs, model, device, crop_size=256, stride=256, batched=True):\n",
    "    \"\"\"\n",
    "    # 0 - x\n",
    "    # 1 - F.hflip(x)\n",
    "    # 2 - F.vflip(x)\n",
    "    # 3 - F.hflip(F.vflip(x))\n",
    "    # 4 - F.rotate(x, 90)\n",
    "    # 5 - F.rotate(F.hflip(x), 90)\n",
    "    # 6 - F.rotate(F.vflip(x), 90)\n",
    "    # 7 - F.rotate(F.hflip(F.vflip(x)), 90)\n",
    "    \"\"\"\n",
    "    # Augment (~1 seconds)\n",
    "    inputs_0 = inputs\n",
    "    inputs_1 = F2.hflip(inputs_0)\n",
    "    inputs_2 = F2.vflip(inputs_0)\n",
    "    inputs_3 = F2.hflip(inputs_2)\n",
    "    inputs_4 = F2.rotate(inputs_0, 90, expand=True)\n",
    "    inputs_5 = F2.hflip(inputs_4)\n",
    "    inputs_6 = F2.vflip(inputs_4)\n",
    "    inputs_7 = F2.hflip(inputs_6)\n",
    "    \n",
    "    # Inference (~100 seconds GPU)\n",
    "    if batched:\n",
    "    \n",
    "        batched_inputs_0 = torch.cat([inputs_0, inputs_1, inputs_2, inputs_3], 0)\n",
    "        batched_inputs_1 = torch.cat([inputs_4, inputs_5, inputs_6, inputs_7], 0)\n",
    "        batched_outputs_0 = strided_predict(batched_inputs_0, model, device, crop_size=crop_size, stride=stride)\n",
    "        batched_outputs_1 = strided_predict(batched_inputs_1, model, device, crop_size=crop_size, stride=stride)\n",
    "        outputs_0 = batched_outputs_0[0].unsqueeze(0)\n",
    "        outputs_1 = batched_outputs_0[1].unsqueeze(0)\n",
    "        outputs_2 = batched_outputs_0[2].unsqueeze(0)\n",
    "        outputs_3 = batched_outputs_0[3].unsqueeze(0)\n",
    "        outputs_4 = batched_outputs_1[0].unsqueeze(0)\n",
    "        outputs_5 = batched_outputs_1[1].unsqueeze(0)\n",
    "        outputs_6 = batched_outputs_1[2].unsqueeze(0)\n",
    "        outputs_7 = batched_outputs_1[3].unsqueeze(0)\n",
    "                    \n",
    "    else:\n",
    "        outputs_0 = strided_predict(inputs_0, model, device, crop_size=crop_size, stride=stride)\n",
    "        outputs_1 = strided_predict(inputs_1, model, device, crop_size=crop_size, stride=stride)\n",
    "        outputs_2 = strided_predict(inputs_2, model, device, crop_size=crop_size, stride=stride)\n",
    "        outputs_3 = strided_predict(inputs_3, model, device, crop_size=crop_size, stride=stride)\n",
    "        outputs_4 = strided_predict(inputs_4, model, device, crop_size=crop_size, stride=stride)\n",
    "        outputs_5 = strided_predict(inputs_5, model, device, crop_size=crop_size, stride=stride)\n",
    "        outputs_6 = strided_predict(inputs_6, model, device, crop_size=crop_size, stride=stride)\n",
    "        outputs_7 = strided_predict(inputs_7, model, device, crop_size=crop_size, stride=stride)\n",
    "    \n",
    "    # Revert augmentation on predictions (~1 seconds)\n",
    "    outputs_1 = F2.hflip(outputs_1)\n",
    "    outputs_2 = F2.vflip(outputs_2)\n",
    "    outputs_3 = F2.vflip(F2.hflip(outputs_3))\n",
    "    outputs_4 = F2.rotate(outputs_4, -90, expand=True)\n",
    "    outputs_5 = F2.rotate(F2.hflip(outputs_5), -90, expand=True)\n",
    "    outputs_6 = F2.rotate(F2.vflip(outputs_6), -90, expand=True)\n",
    "    outputs_7 = F2.rotate(F2.vflip(F2.hflip(outputs_7)), -90, expand=True)\n",
    "    \n",
    "    outputs = outputs_0 + outputs_1 + outputs_2 + outputs_3 + outputs_4 + outputs_5 + outputs_6 + outputs_7\n",
    "    return outputs / 8\n",
    "\n",
    "\n",
    "# 92.516947 seconds - 1 inputs\n",
    "#output_image = test_time_augmentation_predict(original_inputs, model, device)\n",
    "\n",
    "# 109.032667 seconds - 2 inputs \n",
    "# 105.018605 seconds - 3 inputs\n",
    "#output_image = test_time_augmentation_predict(torch.zeros(3,7,2154,2554), model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir =                \"../../data/03_training_data/normalized_bias/valid/input/20x_images\"\n",
    "target_dir =               \"../../data/03_training_data/normalized_bias/valid/targets/20x_images\"\n",
    "dataset = PredctionDataset(\"../../data/03_training_data/normalized_bias/valid/input/20x_images\")\n",
    "\n",
    "\n",
    "inputs, filenames, target_filenames = dataset[0]\n",
    "x = torch.Tensor(inputs).unsqueeze(0)\n",
    "\n",
    "# 350.9 seconds - stride = 128\n",
    "# 47.276119 seconds\n",
    "# 17 seconds if batched\n",
    "output_image = test_time_augmentation_predict(x, model, device, crop_size=256, stride=64)\n",
    "\n",
    "\n",
    "#output_image = strided_predict(x, model, device, crop_size=256, stride=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(input_dir, filenames[0])\n",
    "input_img = cv2.imread(input_path, -1)\n",
    "plt.imshow(input_img)\n",
    "plt.show()\n",
    "plt.imshow(input_img[:256,:256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output_image[0,0])\n",
    "plt.show()\n",
    "plt.imshow(output_image[0,0][:256,:256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = os.path.join(target_dir, target_filenames[1])\n",
    "target_img = cv2.imread(target_path, -1)\n",
    "plt.imshow(target_img)\n",
    "plt.show()\n",
    "plt.imshow(target_img[:256,:256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((torch.Tensor(target_img.astype(np.float))-output_image)[0,0])\n",
    "(torch.Tensor(target_img.astype(np.float))-output_image).abs().mean()\n",
    "\n",
    "# \"Number of TTA\" vs \"MAE\" - Train\n",
    "# 0 - tensor(262.8758)\n",
    "# 1 - tensor(260.5205)\n",
    "# 2 - tensor(256.0114)\n",
    "# 3 - tensor(250.9733)\n",
    "# 4 - tensor(248.4582)\n",
    "# 5 - tensor(247.0434)\n",
    "# 6 - tensor(246.8050)\n",
    "# 7 - tensor(246.5606)\n",
    "\n",
    "# \"Number of TTA\" vs \"MAE\" - Valid\n",
    "# 0 - tensor(299.3842)\n",
    "# 1 - tensor(301.9908)\n",
    "# 2 - tensor(296.3426)\n",
    "# 3 - tensor(296.8527)\n",
    "# 4 - tensor(295.8753)\n",
    "# 5 - tensor(295.5926)\n",
    "# 6 - tensor(294.9396)\n",
    "# 7 - tensor(295.0099) ----- our choice\n",
    "\n",
    "# \"Crop_size\" vs \"MAE\" - Valid, stride=256\n",
    "# 256  - tensor(295.0099)\n",
    "# 512  - tensor(291.7748)\n",
    "# 1024 - tensor(276.7041)\n",
    "# 2048 - tensor(256.1253) ---- our choice\n",
    "\n",
    "# \"stride\" vs \"MAE\" - Valid, stride=256, crop_size=2048\n",
    "# 1    -  -\n",
    "# 2    -  -\n",
    "# 4    -  -\n",
    "# 8    -  -\n",
    "# 16   -  -\n",
    "# 32   -  -\n",
    "# 64   - tensor(292.9769) - 328.620675 s -> \n",
    "# 128  - tensor(293.9679) -\n",
    "# 256  - tensor(295.0099) -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"The effects of Test-time Augmentations (TTA)\", fontsize=20)\n",
    "plt.plot([0,1,2,3,4,5,6,7], [262.8758, 260.5205, 256.0114, 250.9733, 248.4582, 247.0434, 246.8050, 246.5606])\n",
    "plt.plot([0,1,2,3,4,5,6,7], [299.3842, 301.9908, 296.3426, 296.8527, 295.8753, 295.5926, 294.9396, 295.0099])\n",
    "plt.legend([\"Train\", \"Validation\"])\n",
    "plt.xlabel(\"Number of augmentations\")\n",
    "plt.ylabel(\"Mean-absolute error MAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../../src\")\n",
    "\n",
    "import os\n",
    "import random\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.unets import UnetResnet152\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    cfg = {\n",
    "        \"model_params\": {\n",
    "            \"class\": \"UnetResnet152\",\n",
    "        },\n",
    "        \"save_path\": \"weights\",\n",
    "        \"pred_path\": \"../../data/05_saved_models/model_1_resnet152_no_normalization.pth\",\n",
    "        \"epochs\": 400,\n",
    "        \"num_workers\": 16,\n",
    "        \"save_checkpoints\": True,\n",
    "        \"load_checkpoint\": True,#False,\n",
    "\n",
    "        \"train_params\": {\n",
    "            \"batch_size\": 32,\n",
    "            \"shuffle\": True,\n",
    "        },\n",
    "\n",
    "        \"valid_params\": {\n",
    "            \"batch_size\": 32,\n",
    "            \"shuffle\": False,\n",
    "        }\n",
    "        \n",
    "    }\n",
    "\n",
    "    Path(\"output\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    #train_dataset = AstraZenecaDataset(\"../data/training_dataset/train\", transform=training_safe_augmentations)\n",
    "    #valid_dataset = AstraZenecaDataset(\"../data/training_dataset/valid\", transform=None)\n",
    "    \n",
    "    dataset = PredctionDataset(\"../data/03_training_data/normalized_bias/valid\")\n",
    "\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = UnetResnet152(output_channels=1)\n",
    "        \n",
    "    # Load checkpoints \n",
    "    weight_file = os.path.join(cfg[\"pred_path\"])\n",
    "    checkpoint = torch.load(weight_file, map_location=device)\n",
    "    epoch = checkpoint['epoch']\n",
    "    best_valid_loss = checkpoint['best_valid_loss']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"Loading weights: {weight_file}\")\n",
    "    print(f\"Trained for {epoch} epochs\")\n",
    "    print(f\"Best validation loss {best_valid_loss}\")\n",
    "\n",
    "    \n",
    "    model.to(device)\n",
    "    print(f\"Starting from epoch {epoch}\")\n",
    "        \n",
    "    model.eval()        \n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    with tqdm(total=len(dataset), desc=f'Epoch {epoch + 1}/{cfg[\"epochs\"]}', unit='img') as pbar:\n",
    "        for inputs in dataloader:\n",
    "            inputs = inputs.float()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets[:,0].unsqueeze(1).to(device)            \n",
    "            \n",
    "            preds = model(inputs)\n",
    "\n",
    "            pbar.set_postfix(**{'valid loss: ': np.mean(valid_losses)})\n",
    "            pbar.update(inputs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "350899.807 /1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
