{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Jazzy-cats/AstraZenecaHackathon/blob/master/notebooks/AstraZeneca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "_Vg-6YzDc4UV",
    "outputId": "13014828-eb79-48be-8b7a-7f72635d7a59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'AstraZenecaHackathon'...\n",
      "remote: Enumerating objects: 15, done.\u001b[K\n",
      "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
      "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
      "remote: Total 80 (delta 5), reused 6 (delta 1), pack-reused 65\u001b[K\n",
      "Unpacking objects: 100% (80/80), done.\n",
      "Checking out files: 100% (50/50), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Jazzy-cats/AstraZenecaHackathon.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "q-nuUkSIzCM-"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/AstraZenecaHackathon/images_for_preview /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694
    },
    "id": "-WNuAyRCbp4K",
    "outputId": "8e54323a-8e20-4276-a124-2da7774fde33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentations\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/33/1c459c2c9a4028ec75527eff88bc4e2d256555189f42af4baf4d7bd89233/albumentations-0.4.6.tar.gz (117kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 7.6MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.4.1)\n",
      "Collecting imgaug>=0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n",
      "\u001b[K     |████████████████████████████████| 952kB 15.1MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (4.1.2.30)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (2.4.1)\n",
      "Requirement already satisfied, skipping upgrade: scikit-image>=0.14.2 in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (0.16.2)\n",
      "Requirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (7.0.0)\n",
      "Requirement already satisfied, skipping upgrade: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations) (2.5)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations) (4.4.2)\n",
      "Building wheels for collected packages: albumentations\n",
      "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for albumentations: filename=albumentations-0.4.6-cp36-none-any.whl size=65165 sha256=e1928af54cd18d5a6825a74a8583573f7b91ca3733c72ee9dadd309b423f8acf\n",
      "  Stored in directory: /root/.cache/pip/wheels/c7/f4/89/56d1bee5c421c36c1a951eeb4adcc32fbb82f5344c086efa14\n",
      "Successfully built albumentations\n",
      "Installing collected packages: imgaug, albumentations\n",
      "  Found existing installation: imgaug 0.2.9\n",
      "    Uninstalling imgaug-0.2.9:\n",
      "      Successfully uninstalled imgaug-0.2.9\n",
      "  Found existing installation: albumentations 0.1.12\n",
      "    Uninstalling albumentations-0.1.12:\n",
      "      Successfully uninstalled albumentations-0.1.12\n",
      "Successfully installed albumentations-0.4.6 imgaug-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPwt5yZjbEGd"
   },
   "outputs": [],
   "source": [
    "!pip install pylocron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mX80-ycGdc3z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import PIL.Image as Image\n",
    "#from sklearn.preprocessing import PowerTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet152\n",
    "import albumentations as A\n",
    "from fastai.vision.models import DynamicUnet\n",
    "import holocron.models as models  # For UNet++ and UNet+++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asvDRdro5WQA"
   },
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Let's look at the distribution of the pixel values and see if we can transform into normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyvdJb28-RZ9"
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkD-Gx9E6IdQ"
   },
   "outputs": [],
   "source": [
    "input = glob.glob(\"/content/images_for_preview/*/input/*\") # log\n",
    "target1 = glob.glob(\"/content/images_for_preview/*/targets/*A01*\")\n",
    "target2 = glob.glob(\"/content/images_for_preview/*/targets/*A02*\")\n",
    "target3 = glob.glob(\"/content/images_for_preview/*/targets/*A03*\") # log\n",
    "\n",
    "def get_mean_and_std(paths, func=None):\n",
    "    pixel_values = list()\n",
    "    for i in paths:\n",
    "        img = cv2.imread(i, -1)\n",
    "        if func is not None:\n",
    "            img = func(img)\n",
    "        pixel_values.append(img.flatten())\n",
    "    pixel_values = np.concatenate(pixel_values)\n",
    "    return np.mean(pixel_values), np.std(pixel_values)\n",
    "\n",
    "def plot_histogram_w_stats(img_path, mean, std, func=None, ax=None):\n",
    "    img = cv2.imread(img_path, -1)\n",
    "\n",
    "    if func is not None:\n",
    "        img = func(img)\n",
    "    #plt.figure(figsize=(20,6))\n",
    "    ax.hist(img.ravel(), 200)\n",
    "    ax.axvline(x=mean,color='red')\n",
    "    ax.axvline(x=mean+std,color='green')\n",
    "    ax.axvline(x=mean-std,color='green')\n",
    "    #plt.show()\n",
    "\n",
    "def plot_histograms_w_stats(paths, sample_idx=0, transform=None):\n",
    "\n",
    "    fig, axs = plt.subplots(1,2, figsize=(20,6))\n",
    "    fig.suptitle(f\"Pixel value distribution: {paths[0]}\", fontsize=16)\n",
    "    axs[0].set_title('Original data')\n",
    "    axs[1].set_title('After transformation')\n",
    "    axs[0].set_xlabel('Pixel value')\n",
    "    axs[0].set_ylabel('Counts')\n",
    "    axs[1].set_xlabel('Pixel value')\n",
    "    axs[1].set_ylabel('Counts')\n",
    "\n",
    "    mean, std = get_mean_and_std(paths)\n",
    "    print(mean, std)\n",
    "    plot_histogram_w_stats(paths[sample_idx], mean, std, ax=axs[0])\n",
    "\n",
    "    mean, std = get_mean_and_std(paths, func=transform)\n",
    "    print(mean, std)\n",
    "    plot_histogram_w_stats(paths[sample_idx], mean, std, func=transform, ax=axs[1])\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "log_func = lambda x: np.log(x + 1e-3)\n",
    "\n",
    "# 1250.8056198505967 1084.9798588268861\n",
    "# 6.789944517079453 0.8247920659414303 - np.log(x + 1e-3)\n",
    "plot_histograms_w_stats(input, transform=log_func)\n",
    "\n",
    "# 378.4063635628563 486.62929332530007\n",
    "# 5.5090300908038135 0.8687941890567182 - np.log(x + 1e-3)\n",
    "plot_histograms_w_stats(target1, transform=log_func)\n",
    "\n",
    "# 988.0271635755419 898.4957144105864\n",
    "# 27.253351 15.661491 - np.sqrt\n",
    "plot_histograms_w_stats(target2, transform=np.sqrt)\n",
    "\n",
    "# 482.04090429532835 254.4028682542436\n",
    "# 6.054861212228503 0.49684806276264926 - np.log(x + 1e-3)\n",
    "plot_histograms_w_stats(target3, transform=log_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARiA_6keEQOM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXLkdIE96DHG"
   },
   "source": [
    "# Creating a PyTorch Dataset and DataLoader for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0d8jRS_dCEx"
   },
   "outputs": [],
   "source": [
    "def get_image_metadata(path_to_file):\n",
    "    \"\"\"Function for extracting metadata from filename of dataset provided by AstraZeneca.\n",
    "    \n",
    "    B02   - well (row B, column 02) \n",
    "    T0001 - timepoint (irrelevant for this dataset) \n",
    "    F001  - field of view = site \n",
    "    L01   - timeline (irrelevant for this datset) \n",
    "    A01   - action list number (3 fluorescent + 1 brightfield action) \n",
    "    Z01   - 3D z-number (slice number or vertical position) \n",
    "    C01   - imaging channel (1 nuclear, 2 lipids, 3 cytoplasm, 4 brightfield)\n",
    "    \"\"\"\n",
    "    string_arr = os.path.basename(path_to_file).split(\"_\")\n",
    "    row_col = string_arr[-2] # Row and column\n",
    "    timepoint = string_arr[-1][:5]\n",
    "    fied_of_view = string_arr[-1][5:9]\n",
    "    timeline = string_arr[-1][9:12]\n",
    "    action_list_number = string_arr[-1][12:15]\n",
    "    z_number_3d = string_arr[-1][15:18]\n",
    "    imaging_channel = string_arr[-1][18:21]\n",
    "    return {\n",
    "        \"row_col\":       row_col,\n",
    "        #\"timepoint\":     timepoint,  # apparently irrelevant\n",
    "        \"field of view\": fied_of_view,\n",
    "        #\"timeline\": timeline,  # apparently irrelevant\n",
    "        \"action_list_number\": action_list_number,\n",
    "        \"z_number_3d\": z_number_3d,\n",
    "        \"imaging_channel\": imaging_channel,\n",
    "        \"path\": path_to_file\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aD3PUOMdN34"
   },
   "outputs": [],
   "source": [
    "class ExampleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path, crop_size=(256,256), transform=None):\n",
    "        \"\"\"Example dataset for sample images for the Astra Zeneca competition\n",
    "        \n",
    "        Group by row_col and field of view\n",
    "        # row_col\n",
    "        # field of view\n",
    "         \n",
    "        Input and Target share these common values:\n",
    "        - row_col       = sample id? \n",
    "        - field of view = amount of zoom\n",
    "\n",
    "        For identifying INPUT:\n",
    "        - action_list_number A04\n",
    "        - imaging_channel    C04\n",
    "        - z_number_3d        Z01 - Z07\n",
    "\n",
    "        For identifying TARGET:\n",
    "        - action_list_number A01 A02 and A03\n",
    "        - imaging_channel    C01, C02, C03\n",
    "        - z_number_3d        Z01\n",
    "        \"\"\"\n",
    "        self.dataset_path = dataset_path\n",
    "        \n",
    "        dataset_samples = glob.glob(os.path.join(self.dataset_path, \"*/*/Assay*\"))\n",
    "        \n",
    "        dataset_dicts = [get_image_metadata(path) for path in dataset_samples]\n",
    "        \n",
    "        # Group all 7 inputs with all 3 respective targets into variable sample\n",
    "        samples = dict()\n",
    "        for sample_dict in dataset_dicts:\n",
    "            sample_key = (sample_dict[\"row_col\"], sample_dict[\"field of view\"])\n",
    "\n",
    "            if samples.get(sample_key) is None:\n",
    "                samples[sample_key] = {\n",
    "                    \"input\": dict(),\n",
    "                    \"target\": dict()\n",
    "                }\n",
    "\n",
    "            if sample_dict[\"action_list_number\"] == \"A04\": # or sample_dict[\"imaging_channel\"] == \"C04\"\n",
    "                # Is an input\n",
    "                z_number_3d = sample_dict[\"z_number_3d\"]\n",
    "                samples[sample_key][\"input\"][z_number_3d] = sample_dict[\"path\"]\n",
    "            else:\n",
    "                # Is an target\n",
    "                action_list_number = sample_dict[\"action_list_number\"]\n",
    "                samples[sample_key][\"target\"][action_list_number] = sample_dict[\"path\"]\n",
    "\n",
    "        self.samples = list(samples.values())\n",
    "        self.crop_size = crop_size\n",
    "        self.transforms = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return len(self.samples)\n",
    "        return 200\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Modulo\n",
    "        idx = idx % len(self.samples)\n",
    "\n",
    "        sample_dict = self.samples[idx]\n",
    "\n",
    "        w, h = cv2.imread(sample_dict[\"input\"][\"Z01\"], -1).shape\n",
    "        assert self.crop_size[0] <= w\n",
    "        assert self.crop_size[1] <= h\n",
    "\n",
    "        # Select random crop\n",
    "        #c_w, c_h = self.crop_size\n",
    "        #crop_x = np.random.randint(0, w - c_w)\n",
    "        #crop_y = np.random.randint(0, h - c_h)\n",
    "\n",
    "        #input = torch.zeros((7, c_w, c_h))\n",
    "        #output = torch.zeros((3, c_w, c_h))\n",
    "        input = torch.zeros((7, w, h))\n",
    "        output = torch.zeros((3, w, h))\n",
    "        for i, z_number_3d in enumerate([\"Z01\", \"Z02\", \"Z03\", \"Z04\", \"Z05\", \"Z06\", \"Z07\"]):\n",
    "            img_path = sample_dict[\"input\"][z_number_3d]\n",
    "\n",
    "            img = cv2.imread(img_path, -1)\n",
    "            #img = img[crop_x:crop_x+c_w, crop_y:crop_y+c_h]\n",
    "\n",
    "            img = img.astype(np.int32)\n",
    "            input[i] = torch.Tensor(img)\n",
    "\n",
    "        for i, action_list_number in enumerate([\"A01\", \"A02\", \"A03\"]):\n",
    "            img_path = sample_dict[\"target\"][action_list_number]\n",
    "\n",
    "            img = cv2.imread(img_path, -1)\n",
    "            #img = img[crop_x:crop_x+c_w, crop_y:crop_y+c_h]\n",
    "\n",
    "            img = img.astype(np.int32)\n",
    "            output[i] = torch.Tensor(img)\n",
    "\n",
    "        if self.transforms:\n",
    "            for transform in self.transforms:\n",
    "                input, output = transform(input, output)\n",
    "\n",
    "        return input, output\n",
    "\n",
    "x, y = ExampleDataset(\"/content/images_for_preview/\")[0]\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4PN36tzfAmC"
   },
   "source": [
    "# Creating Data Augmentations\n",
    "We will be using the following augmentations\n",
    "\n",
    "Affine transformations\n",
    "- Horizontal/Vertical flipping\n",
    "- Rotation \n",
    "- Rescale and Crop \n",
    "- Shear (Not implemented)\n",
    "\n",
    "Color transformations\n",
    "- ??\n",
    "- Channel shuffle (Not implemented)\n",
    "\n",
    "Distortion transformations\n",
    "- Grid distortion (Not implemented)\n",
    "- Elastic transform (Not implemented)\n",
    "\n",
    "\n",
    "## List from 1st place winner of 2018 Data Science Bowl\n",
    "- Clahe, Sharpen, Emboss\n",
    "- Gaussian Noise\n",
    "- Color to Gray\n",
    "- Inverting - we should not have used it, some images were not predicted correctly on stage2 because of this augmentation\n",
    "- Remapping grayscale images to random color images\n",
    "- Blur, Median Blur, Motion Blur\n",
    "- contrast and brightness\n",
    "- random scale, rotates and flips\n",
    "- Heavy geometric transformations: Elastic Transform, Perspective\n",
    "- Transform, Piecewise Affine transforms, pincushion distortion\n",
    "- Random HSV\n",
    "- Channel shuffle - I guess this one was very important due to the nature of the data\n",
    "- Nucleus copying on images. That created a lot of overlapping nuclei.\n",
    "- It seemed to help networks to learn better borders for overlapping nuclei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZ_TCT_jgBc_"
   },
   "outputs": [],
   "source": [
    "class RandomRotation:\n",
    "    \"\"\"Rotate by one of the given angles.\"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5, angles=30):\n",
    "        self.p = p\n",
    "        self.angles = angles\n",
    "\n",
    "    def __call__(self, input, output):\n",
    "        if random.random() < self.p:\n",
    "            angle = random.uniform(-self.angles, self.angles)\n",
    "\n",
    "            for i in range(len(input)):\n",
    "                values = input[i].getextrema()\n",
    "                avg = int(sum(values)/len(values))\n",
    "                input[i] = TF.rotate(input[i], angle, fill=avg)\n",
    "            for i in range(len(output)):\n",
    "                values = output[i].getextrema()\n",
    "                avg = int(sum(values)/len(values))\n",
    "                output[i] = TF.rotate(output[i], angle, fill=0)\n",
    "\n",
    "        return input, output\n",
    "\n",
    "\n",
    "class HorizontalFlipping:\n",
    "    \"\"\"Flip horizontally.\"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, input, output):\n",
    "        if random.random() < self.p:\n",
    "            input = TF.hflip(input)\n",
    "            output = TF.hflip(output)\n",
    "        return input, output\n",
    "\n",
    "class VerticalFlipping:\n",
    "    \"\"\"Flip vertically.\"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, input, output):\n",
    "        if random.random() < self.p:\n",
    "            input = TF.vflip(input)\n",
    "            output = TF.vflip(output)\n",
    "        return input, output\n",
    "\n",
    "class Normalize:\n",
    "    \"\"\"Normalize values.\"\"\"\n",
    "\n",
    "    def __init__(self, input_stats, output_stats=None):\n",
    "        self.input_mean, self.input_std = input_stats\n",
    "        self.output_mean, self.output_std = output_stats\n",
    "\n",
    "    def __call__(self, input, output):\n",
    "        input = (input - self.input_mean)/self.input_std\n",
    "        output = (output - self.output_mean)/self.output_std\n",
    "        return input, output\n",
    "\n",
    "class DistributionTransform:\n",
    "    \"\"\"Transform distribution to be normally distributed.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, input, output):\n",
    "        input = torch.log(input + 1e-3)\n",
    "        output[0] = torch.log(output[0] + 1e-3)\n",
    "        output[1] = torch.sqrt(output[1])\n",
    "        output[2] = torch.log(output[2] + 1e-3)\n",
    "        return input, output\n",
    "\n",
    "class ToPIL:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, input, output):\n",
    "        pil_input = list()\n",
    "        pil_output = list()\n",
    "        for i in range(len(input)):\n",
    "            pil_input.append(Image.fromarray(np.array(input[i], dtype=np.uint16), \"I;16\"))\n",
    "        for i in range(len(output)):\n",
    "            pil_output.append(Image.fromarray(np.array(output[i], dtype=np.uint16), \"I;16\"))\n",
    "        return pil_input, pil_output\n",
    "\n",
    "class ToTensor:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, input, output):\n",
    "        torch_input = list()\n",
    "        torch_output = list()\n",
    "        for i in input:\n",
    "            torch_input.append(TF.to_tensor(i))\n",
    "        for i in output:\n",
    "            torch_output.append(TF.to_tensor(i))\n",
    "        torch_input = torch.cat(torch_input, 0)\n",
    "        torch_output = torch.cat(torch_output, 0)\n",
    "        return torch_input, torch_output\n",
    "\n",
    "class Crop:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, crop_size, ratio=0.2):\n",
    "        self.crop_size = crop_size\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def __call__(self, input, output):\n",
    "\n",
    "        width, height = input[0].size\n",
    "\n",
    "        factor = 1 + random.uniform(0, self.ratio)\n",
    "        crop_width = self.crop_size[1] * factor\n",
    "        crop_height = self.crop_size[0] * factor\n",
    "\n",
    "        crop_x = np.random.randint(0, width - crop_width)\n",
    "        crop_y = np.random.randint(0, height - crop_height)\n",
    "        \n",
    "        for idx, img in enumerate(input):\n",
    "            input[idx] = TF.crop(img, crop_y, crop_x, crop_height, crop_width)\n",
    "\n",
    "        for idx, img in enumerate(output):\n",
    "            output[idx] = TF.crop(img, crop_y, crop_x, crop_height, crop_width)\n",
    "        return input, output\n",
    "\n",
    "class Resize:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, crop_size):\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "    def __call__(self, input, output):\n",
    "\n",
    "        new_input = torch.zeros((7, self.crop_size[0], self.crop_size[1]))\n",
    "        new_output = torch.zeros((3, self.crop_size[0], self.crop_size[1]))\n",
    "        for idx, img in enumerate(input):\n",
    "            new_input[idx] = torch.Tensor(cv2.resize(img.float().numpy(), self.crop_size))\n",
    "\n",
    "        for idx, img in enumerate(output):\n",
    "            new_output[idx] = torch.Tensor(cv2.resize(img.float().numpy(), self.crop_size))\n",
    "        return new_input, new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aR8IUb8fUN0u"
   },
   "outputs": [],
   "source": [
    "crop = Crop((256,256))\n",
    "resize = Resize((256,256))\n",
    "rotation_transform = RandomRotation()\n",
    "to_pil = ToPIL()\n",
    "to_tensor = ToTensor()\n",
    "hflip_transform = HorizontalFlipping()\n",
    "vflip_transform = VerticalFlipping()\n",
    "dist_transform = DistributionTransform()\n",
    "\n",
    "# Without DistributionTransform\n",
    "input_stats = np.array([1250.8056198505967, 1084.9798588268861])\n",
    "output_stats = np.array([\n",
    "    [378.4063635628563,  988.0271635755419, 482.04090429532835],\n",
    "    [486.62929332530007, 898.4957144105864, 254.4028682542436]\n",
    "])\n",
    "output_stats = np.reshape(output_stats, (2,3,1,1))\n",
    "normalize = Normalize(input_stats, output_stats)\n",
    "\n",
    "data_transforms = A.Compose([\n",
    "  to_pil,\n",
    "  crop,\n",
    "  rotation_transform,\n",
    "  to_tensor,\n",
    "  resize,\n",
    "  hflip_transform,\n",
    "  vflip_transform,\n",
    "  normalize\n",
    "  ])\n",
    "\n",
    "# With DistributionTransform \n",
    "input_stats = np.array([6.789944517079453, 0.8247920659414303])\n",
    "output_stats = np.array([\n",
    "    [5.5090300908038135, 27.253351, 6.054861212228503],\n",
    "    [0.8687941890567182, 15.661491, 0.49684806276264926]\n",
    "])\n",
    "output_stats = np.reshape(output_stats, (2,3,1,1))\n",
    "\n",
    "normalize = Normalize(input_stats, output_stats)\n",
    "\n",
    "data_transforms_2 = A.Compose([\n",
    "  hflip_transform,\n",
    "  vflip_transform,\n",
    "  dist_transform,\n",
    "  normalize\n",
    "  ])\n",
    "\n",
    "\n",
    "x, y = ExampleDataset(\"/content/images_for_preview/\", transform=data_transforms)[0]\n",
    "print(x.shape, y.shape)\n",
    "print(x.mean(), y.mean())\n",
    "print(x.max(), y.max())\n",
    "\n",
    "def post_process(target):\n",
    "    output_stats = torch.Tensor([\n",
    "        [378.4063635628563,  988.0271635755419, 482.04090429532835],\n",
    "        [486.62929332530007, 898.4957144105864, 254.4028682542436]\n",
    "    ]).float()\n",
    "    output_stats = output_stats.view(2,1,3,1,1)\n",
    "    # (batch, channel, width, height)\n",
    "    target = target*output_stats[1] + output_stats[0]\n",
    "\n",
    "    return target.float()\n",
    "\n",
    "def post_process_2(target):\n",
    "    output_stats = torch.Tensor([\n",
    "        [5.5090300908038135, 27.253351, 6.054861212228503],\n",
    "        [0.8687941890567182, 15.661491, 0.49684806276264926]\n",
    "    ]).float()\n",
    "    output_stats = output_stats.view(2,1,3,1,1)\n",
    "    # (batch, channel, width, height)\n",
    "    target = target*output_stats[1] + output_stats[0]\n",
    "\n",
    "    target[:,0] = torch.exp(target[:,0]) - 1e-3\n",
    "    target[:,1] = target[:,1]**2\n",
    "    target[:,2] = torch.exp(target[:,2]) - 1e-3\n",
    "    return target.float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KY05_T1J1t1k"
   },
   "source": [
    "## Test: Confirm Dataset is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRndVWK3duvP"
   },
   "outputs": [],
   "source": [
    "# Confirm that Dataset is working\n",
    "fig, ax = plt.subplots(1,7, figsize=(24,4))\n",
    "fig.suptitle(\"Training samples\", fontsize=40)\n",
    "names = [\"Z1\", \"Z2\", \"Z3\", \"Z4\", \"Z5\", \"Z6\", \"Z7\"]\n",
    "for i, x_sample in enumerate(x):\n",
    "    ax[i].set_title(names[i])\n",
    "    ax[i].imshow(x_sample)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(10,4))\n",
    "fig.suptitle(\"Target samples\", fontsize=40)\n",
    "names = [\"Nuclei\", \"Adipocytes\", \"Cytoplasm\"]\n",
    "for i, x_sample in enumerate(y):\n",
    "    ax[i].imshow(x_sample)\n",
    "    ax[i].set_title(names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCB3uFxYoFWZ"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(x_sample.numpy().ravel(), 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPpy031uu199"
   },
   "outputs": [],
   "source": [
    "dataset = ExampleDataset(\"/content/images_for_preview/\", transform=data_transforms)\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=0)\n",
    "x, y = next(iter(dataloader))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3wTJXxj5fus"
   },
   "source": [
    "# Training with UNet\n",
    "\n",
    "To use UNet, we do the following\n",
    "1. Obtain pretrained UNet model\n",
    "2. Change input layer to fit 7 channels\n",
    "3. Change output layer to output 3 channels\n",
    "\n",
    "\n",
    "## Data Science Bowl 2018 1st winner:\n",
    "- They suggest these following backbones for best performance: \n",
    "  - DPN-92\n",
    "  - Resnet-152\n",
    "  - InceptionResnetV2\n",
    "  - Resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1ZYAVmxk3LG"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"Just a basic UNet model.\n",
    "\n",
    "    https://github.com/milesial/Pytorch-UNet\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=7, output_channels=3):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Get model from PyTorch hub\n",
    "        net = torch.hub.load('milesial/Pytorch-UNet', 'unet_carvana')\n",
    "\n",
    "        # Configure input layer to take 7 channels\n",
    "        net.inc.double_conv[0] = nn.Conv2d(\n",
    "            input_channels, \n",
    "            net.inc.double_conv[0].out_channels, \n",
    "            kernel_size = net.inc.double_conv[0].kernel_size, \n",
    "            stride = net.inc.double_conv[0].stride, \n",
    "            padding = net.inc.double_conv[0].padding\n",
    "            )\n",
    "\n",
    "        # Configure output layer to output 3 channels\n",
    "        net.outc.conv = nn.Conv2d(\n",
    "            net.outc.conv.in_channels,\n",
    "            output_channels,\n",
    "            kernel_size = net.outc.conv.kernel_size,\n",
    "            stride = net.outc.conv.stride\n",
    "        )\n",
    "        self.net = net\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNet3P(nn.Module):\n",
    "    \"\"\"Just a basic UNet+++ model.\n",
    "\n",
    "    https://github.com/frgfm/Holocron\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=7, output_channels=3):\n",
    "        super(UNet3P, self).__init__()\n",
    "\n",
    "        # Get model from Holocron library\n",
    "        net = models.unet3p(pretrained=False)\n",
    "\n",
    "        # Configure input layer to take 7 channels\n",
    "        net.encoders[0][0] = nn.Conv2d(\n",
    "            input_channels, \n",
    "            net.encoders[0][0].out_channels, \n",
    "            kernel_size = net.encoders[0][0].kernel_size, \n",
    "            stride = net.encoders[0][0].stride, \n",
    "            padding = net.encoders[0][0].padding\n",
    "        )\n",
    "\n",
    "        # Configure output layer to output 3 channels\n",
    "        net.classifier = nn.Conv2d(\n",
    "            net.classifier.in_channels,\n",
    "            output_channels,\n",
    "            kernel_size = net.classifier.kernel_size,\n",
    "            stride = net.classifier.stride\n",
    "        )\n",
    "        self.net = net\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNet_with_backbone(nn.Module):\n",
    "    def __init__(self, input_channels=7, output_channels=3):\n",
    "        super(UNet_with_backbone, self).__init__()\n",
    "        backbone = torchvision.models.resnet152(pretrained=True)\n",
    "        backbone.conv1 = nn.Conv2d(\n",
    "            input_channels,\n",
    "            out_channels = backbone.conv1.out_channels,\n",
    "            kernel_size = backbone.conv1.kernel_size,\n",
    "            stride  = backbone.conv1.stride,\n",
    "            bias  = backbone.conv1.bias\n",
    "        )\n",
    "        backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
    "\n",
    "        # Using Fastai API to construct UNet from any encoders (ResNet152 in this case)\n",
    "        UNet = DynamicUnet( \n",
    "            backbone,        # Backbone\n",
    "            output_channels, # Number of classes/channels\n",
    "            (256,256),       # Output shape\n",
    "            norm_type=None\n",
    "        )\n",
    "        self.UNet = UNet\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.UNet(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#models.unetp()  UNet+\n",
    "#models.unetpp() UNet++\n",
    "#models.unet3p() UNet+++\n",
    "\n",
    "# Test inference\n",
    "model = UNet_with_backbone(input_channels=7, output_channels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQZe30lh3aF7"
   },
   "source": [
    "# PyTorch training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d91oW4v1rstg"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1B8Ku52umAq"
   },
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "global_step = 0\n",
    "save_cp = False\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "all_A1_train_losses = list()\n",
    "all_A2_train_losses = list()\n",
    "all_A3_train_losses = list()\n",
    "for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        A1_train_losses = list()\n",
    "        A2_train_losses = list()\n",
    "        A3_train_losses = list()\n",
    "        batch_train_losses = list()\n",
    "        with tqdm(total=len(dataset), desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
    "            for inputs, targets in dataloader:\n",
    "\n",
    "                inputs = inputs.to(device=device, dtype=torch.float32)\n",
    "                targets = targets.to(device, dtype=torch.float32)\n",
    "                \n",
    "                assert not torch.isnan(inputs).any()  # Input\n",
    "                assert not torch.isnan(targets).any() # targets\n",
    "                preds = model(inputs)\n",
    "                assert not torch.isnan(preds).any() # preds\n",
    "                loss = criterion(preds, targets)\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_value_(model.parameters(), 0.1)\n",
    "                optimizer.step()\n",
    "\n",
    "                global_step += 1\n",
    "                pred = post_process(preds.detach().cpu())\n",
    "                targets = post_process(targets.cpu())\n",
    "                comp_loss_A1 = criterion(pred[:,0], targets[:,0]).item()\n",
    "                comp_loss_A2 = criterion(pred[:,1], targets[:,1]).item()\n",
    "                comp_loss_A3 = criterion(pred[:,2], targets[:,2]).item()\n",
    " \n",
    "                A1_train_losses.append(comp_loss_A1)\n",
    "                A2_train_losses.append(comp_loss_A2)\n",
    "                A3_train_losses.append(comp_loss_A3)\n",
    "                batch_train_losses.append(loss.item())\n",
    "                pbar.set_postfix(**{'loss (batch)': np.mean(batch_train_losses), \"A1\": np.mean(A1_train_losses), \"A2\": np.mean(A2_train_losses), \"A3\": np.mean(A3_train_losses)})\n",
    "                pbar.update(inputs.shape[0])\n",
    "\n",
    "                #if global_step % (len(dataset) // (10 * batch_size)) == 0:\n",
    "                #    for tag, value in model.named_parameters():\n",
    "                #        tag = tag.replace('.', '/')\n",
    "                #    val_score = eval_net(model, val_loader, device)\n",
    "                #    scheduler.step(val_score)\n",
    "                #\n",
    "                #    if model.n_classes > 1:\n",
    "                #        logging.info('Validation cross entropy: {}'.format(val_score))\n",
    "                #    else:\n",
    "                #        logging.info('Validation Dice Coeff: {}'.format(val_score))\n",
    "        all_A1_train_losses.append(np.mean(A1_train_losses))\n",
    "        all_A2_train_losses.append(np.mean(A2_train_losses))\n",
    "        all_A3_train_losses.append(np.mean(A3_train_losses))\n",
    "\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(dir_checkpoint)\n",
    "                logging.info('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            torch.save(net.state_dict(),\n",
    "                       dir_checkpoint + f'CP_epoch{epoch + 1}.pth')\n",
    "            logging.info(f'Checkpoint {epoch + 1} saved !')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEMvj928X07y"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(all_A1_train_losses)\n",
    "plt.plot(all_A2_train_losses)\n",
    "plt.plot(all_A3_train_losses)\n",
    "plt.legend([\"A1\"])#, \"A2\", \"A3\"])\n",
    "plt.ylim([50000, 850000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgBX1zB7uuWO"
   },
   "outputs": [],
   "source": [
    "# Sample inference and visualization\n",
    "x, y = ExampleDataset(\"/content/images_for_preview/\", transform=data_transforms)[0]\n",
    "\n",
    "fig, ax = plt.subplots(1,7, figsize=(24,15))\n",
    "names = [\"Z1\", \"Z2\", \"Z3\", \"Z4\", \"Z5\", \"Z6\", \"Z7\"]\n",
    "for i, x_sample in enumerate(x):\n",
    "    ax[i].set_title(names[i])\n",
    "    ax[i].imshow(x_sample)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(10,6))\n",
    "fig.suptitle(\"Target samples\", fontsize=40)\n",
    "names = [\"Nuclei\", \"Adipocytes\", \"Cytoplasm\"]\n",
    "for i, x_sample in enumerate(y):\n",
    "    ax[i].set_title(names[i])\n",
    "    ax[i].imshow(x_sample)\n",
    "\n",
    "model.eval().to(device)\n",
    "preds = model(x.unsqueeze(0).to(device)).cpu()\n",
    "fig, ax = plt.subplots(1,3, figsize=(10,6))\n",
    "fig.suptitle(\"Prediction samples\", fontsize=40)\n",
    "names = [\"Nuclei\", \"Adipocytes\", \"Cytoplasm\"]\n",
    "for i, x_sample in enumerate(preds[0]):\n",
    "    ax[i].set_title(names[i])\n",
    "    ax[i].imshow(x_sample.detach().numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XlBACgIoJwGS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJ4OoD7_KGoO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "AstraZeneca.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
